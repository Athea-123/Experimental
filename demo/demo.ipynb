{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyclense Demo: Cleaning the Beach Dataset\n",
    "\n",
    "This notebook demonstrates how to use the `pyclense` library to build a sequential data cleaning pipeline for the `dataset.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root to path to import pyclense\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from pyclense.base import BaseCleaner\n",
    "from pyclense.standardizer import FormatStandardizer\n",
    "from pyclense.missing import MissingDataCleaner\n",
    "from demo_cleaner import DemoCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Messy Dataset\n",
    "\n",
    "Let's load the `dataset.csv` file from the `data` folder. It contains several data quality issues that we will address with our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original Messy Data ---\n",
      "Loaded 150 rows from ..\\data\\dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Beach / Location Name</th>\n",
       "      <th>Accommodation Name</th>\n",
       "      <th>Rating (Out of 5)</th>\n",
       "      <th>Fee (USD/Night)</th>\n",
       "      <th>Review Date</th>\n",
       "      <th>Written Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Turks and Caicos</td>\n",
       "      <td>Grace Bay Beach</td>\n",
       "      <td>Amanyara</td>\n",
       "      <td>4.9</td>\n",
       "      <td>$1,800</td>\n",
       "      <td>20-Nov-25</td>\n",
       "      <td>Impossibly white sand and water so blue it loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>Praia da Falésia (Algarve)</td>\n",
       "      <td>Pine Cliffs Hotel, a Luxury Collection</td>\n",
       "      <td>4.7</td>\n",
       "      <td>$410</td>\n",
       "      <td>10/15/2025</td>\n",
       "      <td>The dramatic golden cliffs meet the azure Atla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>Tulum Beach</td>\n",
       "      <td>Be Tulum Hotel</td>\n",
       "      <td>4.5</td>\n",
       "      <td>$650</td>\n",
       "      <td>1-Dec-25</td>\n",
       "      <td>Chic, bohemian luxury right on the white sands...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Philippines</td>\n",
       "      <td>El Nido (Palawan)</td>\n",
       "      <td>Amanpulo</td>\n",
       "      <td>4.8</td>\n",
       "      <td>$2,000</td>\n",
       "      <td>11/12/2025</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>Boulders Beach (Cape Town)</td>\n",
       "      <td>Tintswalo at Boulders Beach</td>\n",
       "      <td>4.8</td>\n",
       "      <td>$580</td>\n",
       "      <td>5-Oct-25</td>\n",
       "      <td>Sharing the sand with African Penguins makes t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Country       Beach / Location Name  \\\n",
       "0  Turks and Caicos             Grace Bay Beach   \n",
       "1          Portugal  Praia da Falésia (Algarve)   \n",
       "2            Mexico                 Tulum Beach   \n",
       "3       Philippines           El Nido (Palawan)   \n",
       "4      South Africa  Boulders Beach (Cape Town)   \n",
       "\n",
       "                       Accommodation Name  Rating (Out of 5) Fee (USD/Night)  \\\n",
       "0                                Amanyara                4.9         $1,800    \n",
       "1  Pine Cliffs Hotel, a Luxury Collection                4.7           $410    \n",
       "2                          Be Tulum Hotel                4.5           $650    \n",
       "3                                Amanpulo                4.8         $2,000    \n",
       "4             Tintswalo at Boulders Beach                4.8           $580    \n",
       "\n",
       "  Review Date                                    Written Review   \n",
       "0   20-Nov-25  Impossibly white sand and water so blue it loo...  \n",
       "1  10/15/2025  The dramatic golden cliffs meet the azure Atla...  \n",
       "2    1-Dec-25  Chic, bohemian luxury right on the white sands...  \n",
       "3  11/12/2025                                                NaN  \n",
       "4    5-Oct-25  Sharing the sand with African Penguins makes t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from the data folder\n",
    "data_path = os.path.join('..', 'data', 'dataset.csv')\n",
    "df_messy = pd.read_csv(data_path)\n",
    "\n",
    "print('--- Original Messy Data ---')\n",
    "print(f\"Loaded {len(df_messy)} rows from {data_path}\")\n",
    "df_messy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building and Running the Pipeline\n",
    "\n",
    "We will now apply our cleaners sequentially. Each cleaner takes a DataFrame and returns a cleaned one, making the process clear and easy to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Cleaning Process ---\n",
      "\n",
      "--- Running DemoCleaner ---\n",
      "Cleaned column names: ['country', 'beach_location_name', 'accommodation_name', 'rating_out_of_5_', 'fee_usd_night_', 'review_date', 'written_review']\n",
      "Removed 11 duplicate rows.\n",
      "Filled missing 'written_review' values.\n",
      "--- DemoCleaner Finished ---\n",
      "[Standardizer] Processed 1 date column(s) and relevant text columns.\n",
      "[MissingDataCleaner] Dropped 0 rows with missing values in ['accommodation_name'].\n",
      "\n",
      "--- Final Cleaned Data ---\n"
     ]
    }
   ],
   "source": [
    "print('--- Starting Cleaning Process ---')\n",
    "\n",
    "# Start with a copy of the messy data\n",
    "df_clean = df_messy.copy()\n",
    "\n",
    "# 1. Use the custom DemoCleaner to perform initial cleaning on the dataset.\n",
    "# This handles column names, fees, duplicates, and fills missing reviews.\n",
    "df_clean = DemoCleaner(df_clean).clean()\n",
    "\n",
    "# 2. Use the generic FormatStandardizer to fix date formats and clean text fields.\n",
    "# The 'written_review' column contains emojis and special characters to be removed.\n",
    "df_clean = FormatStandardizer(df_clean, date_cols=['review_date'], text_cols=['written_review', 'beach_location_name']).clean()\n",
    "\n",
    "# 3. Use the MissingDataCleaner to drop rows where critical data is still missing.\n",
    "# We'll drop rows that don't have an accommodation name.\n",
    "df_clean = MissingDataCleaner(df_clean, subset=['accommodation_name']).clean()\n",
    "\n",
    "print('\\n--- Final Cleaned Data ---')\n",
    "if df_clean is not None:\n",
    "\tdf_clean.head()\n",
    "else:\n",
    "\tprint(\"No data to display: cleaning pipeline returned None.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving the Result\n",
    "\n",
    "Finally, we can save the cleaned data using the helper method from `BaseCleaner`. We can instantiate any concrete cleaner with the final DataFrame to access the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../data/cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Use a concrete cleaner class for saving\n",
    "final_cleaner = DemoCleaner(df_clean)\n",
    "final_cleaner.save_data('../data/cleaned_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
